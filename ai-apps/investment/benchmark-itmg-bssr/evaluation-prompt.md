I conducted a benchmark test on several LLMs using the same prompt and toolset. However, due to differences in tool-calling behavior, some of the outputs may contain missing or skipped data. Your task is to evaluate the performance of each model based on:

1. **Technical Accuracy** — How well the model handles tool usage, data extraction, computations, and factual precision.
2. **Analytical Quality** — The depth of reasoning, structure of conclusions, and ability to interpret results meaningfully.
3. **Correctness of Findings** — Whether the final answers properly reflect the data and tools used, without errors or omissions.

You should **not** evaluate writing quality, style, tone, or presentation. Focus exclusively on the correctness and substance of the analysis.

Each model’s output is provided in a separate attachment, and the model name corresponds to the filename.

### Deliverables

1. A brief evaluation report for each model (organized by model name).
2. An overview section that includes:

   * A ranked list of the models from best to worst performance.
   * Clear justification for the ranking based on the criteria above.

If any output appears incomplete due to skipped tool calls, include this as part of the evaluation.
